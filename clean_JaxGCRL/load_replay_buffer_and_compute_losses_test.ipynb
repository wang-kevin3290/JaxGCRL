{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a75aed-e853-4bb5-8cb7-c837876ddd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = '/scratch/gpfs/kw6487/JaxGCRL/clean_JaxGCRL/runs/ant_2_20250105-130808'\n",
    "params_pkl = 'final.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3debb1-5153-4540-a030-62ef439cd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_path = f'{run_dir}/final_buffer.pkl'\n",
    "args_path = f'{run_dir}/args.pkl'\n",
    "params_path = f'{run_dir}/{params_pkl}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ccd6ce-c02d-41b2-99ad-cd6bb919917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT STUFF, LOAD PKL, COPY DEFINITIONS (Must be interleaved)\n",
    "import pickle\n",
    "import os\n",
    "import jax\n",
    "import flax\n",
    "import tyro\n",
    "import time\n",
    "import optax\n",
    "import wandb\n",
    "import pickle\n",
    "import random\n",
    "import wandb_osh\n",
    "import numpy as np\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from brax import envs\n",
    "from etils import epath\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple, Any\n",
    "from wandb_osh.hooks import TriggerWandbSyncHook\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.linen.initializers import variance_scaling\n",
    "from brax.io import html\n",
    "from brax.io import model\n",
    "\n",
    "from evaluator import CrlEvaluator\n",
    "from buffer import TrajectoryUniformSamplingQueue\n",
    "from memory_bank import MemoryBank, MemoryBankState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd23d71-994e-4bcd-bd0c-fcac49d1a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(buffer_path, 'rb') as f:\n",
    "    buffer_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27812d91-d21d-4798-95b8-2418f5a1fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COPY OVER DEFINITIONS\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"train\" # os.path.basename(__file__)[: -len(\".py\")]\n",
    "    seed: int = random.randint(1, 1000) # 16\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"clean_JaxGCRL_test\"\n",
    "    wandb_entity: str = 'wang-kevin3290-princeton-university'\n",
    "    wandb_mode: str = 'offline'\n",
    "    wandb_dir: str = '.'\n",
    "    wandb_group: str = '.'\n",
    "    capture_vis: bool = True\n",
    "    vis_length: int = 1000\n",
    "    checkpoint: bool = True\n",
    "\n",
    "    #environment specific arguments\n",
    "    env_id: str = \"humanoid\" # \"ant_push\" \"ant_hardest_maze\" \"ant_big_maze\" \"humanoid\" \"ant\"\n",
    "    episode_length: int = 1000\n",
    "    # to be filled in runtime\n",
    "    obs_dim: int = 0\n",
    "    goal_start_idx: int = 0\n",
    "    goal_end_idx: int = 0\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    total_env_steps: int = 100000000 # 50000000\n",
    "    num_epochs: int = 100 # 50\n",
    "    num_envs: int = 512\n",
    "    eval_env_id: str = \"\"\n",
    "    num_eval_envs: int = 128\n",
    "    actor_lr: float = 3e-4\n",
    "    critic_lr: float = 3e-4\n",
    "    alpha_lr: float = 3e-4\n",
    "    batch_size: int = 256\n",
    "    gamma: float = 0.99\n",
    "    logsumexp_penalty_coeff: float = 0.1\n",
    "    \n",
    "    #adding in a batch_size_multiplier argument for critic vs. actor batch size\n",
    "    critic_batch_size_multiplier: float = 1.0 #this has to be less than or equal to 1\n",
    "    actor_batch_size_multiplier: float = 1.0 #this has to be less than 1\n",
    "\n",
    "    max_replay_size: int = 10000\n",
    "    min_replay_size: int = 1000\n",
    "    \n",
    "    unroll_length: int  = 62\n",
    "    \n",
    "    # ADDING IN A NETWORK WIDTH ARGUMENT\n",
    "    same_network_width: int = 0\n",
    "    network_width: int = 256\n",
    "    critic_network_width: int = 256\n",
    "    actor_network_width: int = 256\n",
    "    actor_depth: int = 4\n",
    "    critic_depth: int = 4\n",
    "    actor_skip_connections: int = 0 # 0 for no skip connections, >= 0 means the frequency of skip connections (every X layers)\n",
    "    critic_skip_connections: int = 0 # 0 for no skip connections, >= 0 means the frequency of skip connections (every X layers)\n",
    "    \n",
    "    num_episodes_per_env: int = 1 #the number of episodes to sample from each env when sampling data \n",
    "    #(to ensure number of batches is consistent as increase batch_size; for now, just a bandaid fix)\n",
    "    # should be something like batch_size / 256\n",
    "    training_steps_multiplier: int = 1 #should have the same effect as num_episodes_per_env, hmmm\n",
    "    use_all_batches: int = 0 # if 1, use all batches; if 0, use a random subset of batches\n",
    "    num_sgd_batches_per_training_step: int = 800 # this parameter so as to hold the number of batches constant (no matter batch_size, etc)\n",
    "    \n",
    "    mrn: int = 0\n",
    "    memory_bank: int = 0\n",
    "    memory_bank_size: int = batch_size # this can be modified too\n",
    "    \n",
    "    batchdiv2: int = 0 \n",
    "    # if 1, freeze gradients for second half of batch\n",
    "    # if 2, split in half along sa and freeze second half of g (Eysenbach ablation, remember it's forward loss)\n",
    "    #\n",
    "    # use batch_size * 2 and split in half and freeze gradients and all that (Eysenbach ablation), does not \n",
    "    # TODO: if 2, modifies actor such that it uses the half batch size (isolate for critic ablation)\n",
    "    # can add 3, 4, etc (if diff between 1 and 2, maybe for batch_size ablation we need to have separate for actor and critic)\n",
    "    # add more for instead of discarding second half, just freeze gradients for second half so symmetric with first\n",
    "    \n",
    "    eval_actor: int = 0\n",
    "    # if 0, use deterministic actor for evaluation\n",
    "    # if 1, use stochastic actor for evaluation\n",
    "    # if 2, sample two actions and take the one with the higher Q value\n",
    "    # if K >= 2, sample K actions and take the one with the highest Q value\n",
    "    expl_actor: int = 1\n",
    "    # if 0, use deterministic actor for exploration/collecting data\n",
    "    # if 1, use stochastic actor for exploration/collecting data\n",
    "    # if 2, sample two actions and take the one with the higher Q value\n",
    "    # if K >= 2, sample K actions and take the one with the highest Q value\n",
    "    \n",
    "    entropy_param: float = 0.5\n",
    "    disable_entropy: int = 0\n",
    "    \n",
    "    use_relu: int = 0\n",
    "    \n",
    "    resnet: str = \"noishmistake4_nodense\"\n",
    "    \n",
    "    num_render: int = 10\n",
    "    \n",
    "    \n",
    "    \n",
    "    # to be filled in runtime\n",
    "    env_steps_per_actor_step : int = 0\n",
    "    \"\"\"number of env steps per actor step (computed in runtime)\"\"\"\n",
    "    num_prefill_env_steps : int = 0\n",
    "    \"\"\"number of env steps to fill the buffer before starting training (computed in runtime)\"\"\"\n",
    "    num_prefill_actor_steps : int = 0\n",
    "    \"\"\"number of actor steps to fill the buffer before starting training (computed in runtime)\"\"\"\n",
    "    num_training_steps_per_epoch : int = 0\n",
    "    \"\"\"the number of training steps per epoch(computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd34189-8e03-4289-bd63-6d3461b2fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args_path, 'rb') as f:\n",
    "    args = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ea5be8-bbda-44e6-8cf1-accd0780ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id=args.env_id):\n",
    "        print(f\"making env with env_id: {env_id}\", flush=True)\n",
    "        if env_id == \"reacher\":\n",
    "            from envs.reacher import Reacher\n",
    "            env = Reacher(\n",
    "                backend=\"spring\",\n",
    "            )\n",
    "            args.obs_dim = 10\n",
    "            args.goal_start_idx = 4\n",
    "            args.goal_end_idx = 7\n",
    "        elif env_id == \"pusher\":\n",
    "            from envs.pusher import Pusher\n",
    "            env = Pusher(\n",
    "                backend=\"spring\",\n",
    "            )\n",
    "            args.obs_dim = 20\n",
    "            args.goal_start_idx = 10\n",
    "            args.goal_end_idx = 13\n",
    "        elif env_id == \"ant\":\n",
    "            from envs.ant import Ant\n",
    "            env = Ant(\n",
    "                backend=\"spring\",\n",
    "                exclude_current_positions_from_observation=False,\n",
    "                terminate_when_unhealthy=True,\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 29\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 2\n",
    "\n",
    "        elif \"ant\" in env_id and \"maze\" in env_id: #needed the add the ant check to differentiate with humanoid maze\n",
    "            if \"gen\" not in env_id:\n",
    "                from envs.ant_maze import AntMaze\n",
    "                env = AntMaze(\n",
    "                    backend=\"spring\",\n",
    "                    exclude_current_positions_from_observation=False,\n",
    "                    terminate_when_unhealthy=True,\n",
    "                    maze_layout_name=env_id[4:]\n",
    "                )\n",
    "\n",
    "                args.obs_dim = 29\n",
    "                args.goal_start_idx = 0\n",
    "                args.goal_end_idx = 2\n",
    "            else:\n",
    "                from envs.ant_maze_generalization import AntMazeGeneralization\n",
    "                gen_idx = env_id.find(\"gen\")\n",
    "                maze_layout_name = env_id[4:gen_idx-1]\n",
    "                generalization_config = env_id[gen_idx+4:]\n",
    "                print(f\"maze_layout_name: {maze_layout_name}, generalization_config: {generalization_config}\", flush=True)\n",
    "                env = AntMazeGeneralization(\n",
    "                    backend=\"spring\",\n",
    "                    exclude_current_positions_from_observation=False,\n",
    "                    terminate_when_unhealthy=True,\n",
    "                    maze_layout_name=maze_layout_name,\n",
    "                    generalization_config=generalization_config\n",
    "                )\n",
    "\n",
    "                args.obs_dim = 29\n",
    "                args.goal_start_idx = 0\n",
    "                args.goal_end_idx = 2\n",
    "        \n",
    "        elif env_id == \"ant_ball\":\n",
    "            from envs.ant_ball import AntBall\n",
    "            env = AntBall(\n",
    "                backend=\"spring\",\n",
    "                exclude_current_positions_from_observation=False,\n",
    "                terminate_when_unhealthy=True,\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 31\n",
    "            args.goal_start_idx = 28\n",
    "            args.goal_end_idx = 30\n",
    "\n",
    "        elif env_id == \"ant_push\":\n",
    "            from envs.ant_push import AntPush\n",
    "            env = AntPush(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 31\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 2\n",
    "            \n",
    "        elif env_id == \"humanoid\":\n",
    "            from envs.humanoid import Humanoid\n",
    "            env = Humanoid(\n",
    "                backend=\"spring\",\n",
    "                exclude_current_positions_from_observation=False,\n",
    "                terminate_when_unhealthy=True,\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 268\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "            \n",
    "        elif \"humanoid\" in env_id and \"maze\" in env_id:\n",
    "            from envs.humanoid_maze import HumanoidMaze\n",
    "            env = HumanoidMaze(\n",
    "                backend=\"spring\",\n",
    "                maze_layout_name=env_id[9:]\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 268\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "\n",
    "            \n",
    "        elif env_id == \"arm_reach\":\n",
    "            from envs.manipulation.arm_reach import ArmReach\n",
    "            env = ArmReach(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 13\n",
    "            args.goal_start_idx = 7\n",
    "            args.goal_end_idx = 10\n",
    "            \n",
    "        elif env_id == \"arm_binpick_easy\":\n",
    "            from envs.manipulation.arm_binpick_easy import ArmBinpickEasy\n",
    "            env = ArmBinpickEasy(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 17\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "            \n",
    "        elif env_id == \"arm_binpick_hard\":\n",
    "            from envs.manipulation.arm_binpick_hard import ArmBinpickHard\n",
    "            env = ArmBinpickHard(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 17\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "            \n",
    "        elif env_id == \"arm_binpick_easy_EEF\":\n",
    "            from envs.manipulation.arm_binpick_easy_EEF import ArmBinpickEasyEEF\n",
    "            env = ArmBinpickEasyEEF(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 11\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "        \n",
    "        elif \"arm_grasp\" in env_id: # either arm_grasp or arm_grasp_0.5, etc\n",
    "            from envs.manipulation.arm_grasp import ArmGrasp\n",
    "            cube_noise_scale = float(env_id[10:]) if len(env_id) > 9 else 0.3\n",
    "            env = ArmGrasp(\n",
    "                cube_noise_scale=cube_noise_scale,\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 23\n",
    "            args.goal_start_idx = 16\n",
    "            args.goal_end_idx = 23\n",
    "        \n",
    "        elif env_id == \"arm_push_easy\":\n",
    "            from envs.manipulation.arm_push_easy import ArmPushEasy\n",
    "            env = ArmPushEasy(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 17\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "        \n",
    "        elif env_id == \"arm_push_hard\":\n",
    "            from envs.manipulation.arm_push_hard import ArmPushHard\n",
    "            env = ArmPushHard(\n",
    "                backend=\"mjx\",\n",
    "            )\n",
    "\n",
    "            args.obs_dim = 17\n",
    "            args.goal_start_idx = 0\n",
    "            args.goal_end_idx = 3\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7077c4-0808-438f-8d2a-821a24d5cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecun_unfirom = variance_scaling(1/3, \"fan_in\", \"uniform\")\n",
    "bias_init = nn.initializers.zeros\n",
    "def residual_block(x, width, normalize, activation):\n",
    "    identity = x\n",
    "    x = nn.Dense(width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "    x = normalize(x)\n",
    "    x = activation(x)\n",
    "    x = nn.Dense(width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "    x = normalize(x)\n",
    "    x = activation(x)\n",
    "    x = nn.Dense(width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "    x = normalize(x)\n",
    "    x = activation(x)\n",
    "    x = nn.Dense(width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "    x = normalize(x)\n",
    "    x = activation(x)\n",
    "    x = x + identity\n",
    "    return x\n",
    "\n",
    "class SA_encoder(nn.Module):\n",
    "    norm_type = \"layer_norm\"\n",
    "    network_width: int = 1024\n",
    "    network_depth: int = 4\n",
    "    skip_connections: int = 0\n",
    "    use_relu: int = 0\n",
    "    @nn.compact\n",
    "    def __call__(self, s: jnp.ndarray, a: jnp.ndarray):\n",
    "\n",
    "        lecun_unfirom = variance_scaling(1/3, \"fan_in\", \"uniform\")\n",
    "        bias_init = nn.initializers.zeros\n",
    "        \n",
    "        if self.norm_type == \"layer_norm\":\n",
    "            normalize = lambda x: nn.LayerNorm()(x)\n",
    "        else:\n",
    "            normalize = lambda x: x\n",
    "        \n",
    "        if self.use_relu:\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.swish\n",
    "            \n",
    "        x = jnp.concatenate([s, a], axis=-1)\n",
    "        #Initial layer\n",
    "        x = nn.Dense(self.network_width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        x = normalize(x)\n",
    "        x = activation(x)\n",
    "        #Residual blocks\n",
    "        for i in range(self.network_depth // 4):\n",
    "            x = residual_block(x, self.network_width, normalize, activation)\n",
    "        #Final layer\n",
    "        x = nn.Dense(64, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        return x\n",
    "    \n",
    "class G_encoder(nn.Module):\n",
    "    norm_type = \"layer_norm\"\n",
    "    network_width: int = 1024\n",
    "    network_depth: int = 4\n",
    "    skip_connections: int = 0\n",
    "    use_relu: int = 0\n",
    "    @nn.compact\n",
    "    def __call__(self, g: jnp.ndarray):\n",
    "\n",
    "        lecun_unfirom = variance_scaling(1/3, \"fan_in\", \"uniform\")\n",
    "        bias_init = nn.initializers.zeros\n",
    "\n",
    "        if self.norm_type == \"layer_norm\":\n",
    "            normalize = lambda x: nn.LayerNorm()(x)\n",
    "        else:\n",
    "            normalize = lambda x: x\n",
    "        \n",
    "        if self.use_relu:\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.swish\n",
    "        \n",
    "        x = g\n",
    "        #Initial layer\n",
    "        x = nn.Dense(self.network_width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        x = normalize(x)\n",
    "        x = activation(x)\n",
    "        #Residual blocks\n",
    "        for i in range(self.network_depth // 4):\n",
    "            x = residual_block(x, self.network_width, normalize, activation)\n",
    "        #Final layer\n",
    "        x = nn.Dense(64, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_size: int\n",
    "    norm_type = \"layer_norm\"\n",
    "    network_width: int = 1024\n",
    "    network_depth: int = 4\n",
    "    skip_connections: int = 0 # 0 for no skip connections, >= 0 means the frequency of skip connections (every X layers)\n",
    "    use_relu: int = 0\n",
    "    LOG_STD_MAX = 2\n",
    "    LOG_STD_MIN = -5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.norm_type == \"layer_norm\":\n",
    "            normalize = lambda x: nn.LayerNorm()(x)\n",
    "        else:\n",
    "            normalize = lambda x: x\n",
    "            \n",
    "        if self.use_relu:\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.swish\n",
    "\n",
    "        lecun_unfirom = variance_scaling(1/3, \"fan_in\", \"uniform\")\n",
    "        bias_init = nn.initializers.zeros\n",
    "        \n",
    "        #Initial layer\n",
    "        x = nn.Dense(self.network_width, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        x = normalize(x)\n",
    "        x = activation(x)\n",
    "        #Residual blocks\n",
    "        for i in range(self.network_depth // 4):\n",
    "            x = residual_block(x, self.network_width, normalize, activation)\n",
    "        #Final layer\n",
    "        # x = nn.Dense(64, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "\n",
    "        mean = nn.Dense(self.action_size, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        log_std = nn.Dense(self.action_size, kernel_init=lecun_unfirom, bias_init=bias_init)(x)\n",
    "        \n",
    "        log_std = nn.tanh(log_std)\n",
    "        log_std = self.LOG_STD_MIN + 0.5 * (self.LOG_STD_MAX - self.LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f56634-0b3c-4951-97f7-b16275f718e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making env with env_id: ant\n",
      "data_size: 43\n"
     ]
    }
   ],
   "source": [
    "#Recreate replay buffer\n",
    "env = make_env()\n",
    "env = envs.training.wrap(\n",
    "    env,\n",
    "    episode_length=args.episode_length,\n",
    ")\n",
    "\n",
    "obs_size = env.observation_size\n",
    "action_size = env.action_size\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    \"\"\"Container for a transition\"\"\"\n",
    "    observation: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    discount: jnp.ndarray\n",
    "    extras: jnp.ndarray = ()\n",
    "\n",
    "# Recreate the buffer object\n",
    "dummy_obs = jnp.zeros((obs_size,))\n",
    "dummy_action = jnp.zeros((action_size,))\n",
    "\n",
    "dummy_transition = Transition(\n",
    "    observation=dummy_obs,\n",
    "    action=dummy_action,\n",
    "    reward=0.0,\n",
    "    discount=0.0,\n",
    "    extras={\n",
    "        \"state_extras\": {\n",
    "            \"truncation\": 0.0,\n",
    "            \"seed\": 0.0,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "replay_buffer = TrajectoryUniformSamplingQueue(\n",
    "    max_replay_size=buffer_data['max_replay_size'],\n",
    "    dummy_data_sample=dummy_transition,  # You'll need to recreate this\n",
    "    sample_batch_size=buffer_data['batch_size'],\n",
    "    num_envs=buffer_data['num_envs'],\n",
    "    episode_length=buffer_data['episode_length'],\n",
    ")\n",
    "\n",
    "buffer_state = buffer_data['buffer_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea73607-920c-45cd-838a-5adaa51223bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10025478-af8d-436d-8d0b-bf7eb670e1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f713b9-8380-4b23-93af-4b8f5efad631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f6d03d-f3aa-4475-ae50-c70e7c026be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.load_params(params_path)\n",
    "alpha_params, actor_params, critic_params = params\n",
    "sa_encoder_params, g_encoder_params = critic_params['sa_encoder'], critic_params['g_encoder']\n",
    "\n",
    "\n",
    "actor = Actor(action_size=action_size, network_width=args.actor_network_width, network_depth=args.actor_depth, skip_connections=args.actor_skip_connections, use_relu=args.use_relu)\n",
    "sa_encoder = SA_encoder(network_width=args.critic_network_width, network_depth=args.critic_depth, skip_connections=args.critic_skip_connections, use_relu=args.use_relu)\n",
    "g_encoder = G_encoder(network_width=args.critic_network_width, network_depth=args.critic_depth, skip_connections=args.critic_skip_connections, use_relu=args.use_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d34986-9396-40ae-b251-a6395eb63ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb7555e9-4319-469e-8781-831ed81aa87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting transitions from buffer...\n",
      "buffer_state.data[:, envs_idxs, :].shape: (10000, 512, 43)\n",
      "Extracted transitions shape: (511488, 31)\n"
     ]
    }
   ],
   "source": [
    "#extract all transitions from replay buffer\n",
    "# After loading buffer_state from pickle file\n",
    "def extract_all_transitions(buffer_state, replay_buffer, args):\n",
    "    \"\"\"Extract all transitions from the buffer.\"\"\"\n",
    "    print(\"Extracting transitions from buffer...\", flush=True)\n",
    "    \n",
    "    # Sample transitions from buffer (similar to training code)\n",
    "    transitions_list = []\n",
    "    temp_buffer_state = buffer_state  # Create temporary copy to not modify original\n",
    "    \n",
    "    for _ in range(args.num_episodes_per_env):\n",
    "        temp_buffer_state, new_transitions = replay_buffer.sample(temp_buffer_state)\n",
    "        transitions_list.append(new_transitions)\n",
    "\n",
    "    # Concatenate all sampled transitions\n",
    "    transitions = jax.tree_util.tree_map(\n",
    "        lambda *arrays: jnp.concatenate(arrays, axis=0),\n",
    "        *transitions_list\n",
    "    )\n",
    "\n",
    "    # Process transitions (same as in training)\n",
    "    key = jax.random.PRNGKey(0)  # or any seed\n",
    "    batch_keys = jax.random.split(key, transitions.observation.shape[0])\n",
    "    transitions = jax.vmap(TrajectoryUniformSamplingQueue.flatten_crl_fn, in_axes=(None, 0, 0))(\n",
    "        (args.gamma, args.obs_dim, args.goal_start_idx, args.goal_end_idx), \n",
    "        transitions, \n",
    "        batch_keys\n",
    "    )\n",
    "    \n",
    "    transitions = jax.tree_util.tree_map(\n",
    "        lambda x: jnp.reshape(x, (-1,) + x.shape[2:], order=\"F\"),\n",
    "        transitions,\n",
    "    )\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "# Usage:\n",
    "transitions = extract_all_transitions(buffer_state, replay_buffer, args)\n",
    "print(f\"Extracted transitions shape: {transitions.observation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40f9f17-8a2a-4e81-b762-907bc7c715ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transitions: 511488\n",
      "Number of batches: 999\n"
     ]
    }
   ],
   "source": [
    "# Split transitions into batches\n",
    "num_transitions = len(transitions.observation)\n",
    "num_batches = num_transitions // args.batch_size\n",
    "\n",
    "print(f\"Total transitions: {num_transitions}\")\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "\n",
    "# Reshape transitions into batches\n",
    "batched_transitions = jax.tree_util.tree_map(\n",
    "    lambda x: x[:num_batches * args.batch_size].reshape(num_batches, args.batch_size, *x.shape[1:]),\n",
    "    transitions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb08602-7a27-431b-9301-f9850517e86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e5099c-f88c-4153-aea9-bed000b4130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute losses\n",
    "def compute_critic_loss(transitions, critic_params):\n",
    "    sa_encoder_params, g_encoder_params = critic_params[\"sa_encoder\"], critic_params[\"g_encoder\"]\n",
    "    \n",
    "    obs = transitions.observation[:, :args.obs_dim]\n",
    "    action = transitions.action\n",
    "    \n",
    "    sa_repr = sa_encoder.apply(sa_encoder_params, obs, action)\n",
    "    g_repr = g_encoder.apply(g_encoder_params, transitions.observation[:, args.obs_dim:])\n",
    "    \n",
    "    logits = -jnp.sqrt(jnp.sum((sa_repr[:, None, :] - g_repr[None, :, :]) ** 2, axis=-1))\n",
    "    critic_loss = -jnp.mean(jnp.diag(logits) - jax.nn.logsumexp(logits, axis=1))\n",
    "    \n",
    "    return critic_loss\n",
    "\n",
    "def compute_actor_loss(transitions, actor_params, critic_params, alpha):\n",
    "    obs = transitions.observation\n",
    "    state = obs[:, :args.obs_dim]\n",
    "    future_state = transitions.extras[\"future_state\"]\n",
    "    goal = future_state[:, args.goal_start_idx : args.goal_end_idx]\n",
    "    observation = jnp.concatenate([state, goal], axis=1)\n",
    "\n",
    "    means, log_stds = actor.apply(actor_params, observation)\n",
    "    stds = jnp.exp(log_stds)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x_ts = means + stds * jax.random.normal(key, shape=means.shape)\n",
    "    action = nn.tanh(x_ts)\n",
    "    log_prob = jax.scipy.stats.norm.logpdf(x_ts, loc=means, scale=stds)\n",
    "    log_prob -= jnp.log((1 - jnp.square(action)) + 1e-6)\n",
    "    log_prob = log_prob.sum(-1)\n",
    "\n",
    "    sa_encoder_params, g_encoder_params = critic_params[\"sa_encoder\"], critic_params[\"g_encoder\"]\n",
    "    sa_repr = sa_encoder.apply(sa_encoder_params, state, action)\n",
    "    g_repr = g_encoder.apply(g_encoder_params, goal)\n",
    "\n",
    "    qf_pi = -jnp.sqrt(jnp.sum((sa_repr - g_repr) ** 2, axis=-1))\n",
    "\n",
    "    if args.disable_entropy:\n",
    "        actor_loss = -jnp.mean(qf_pi)\n",
    "    else:\n",
    "        actor_loss = jnp.mean(jnp.exp(alpha) * log_prob - qf_pi)\n",
    "    \n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae480b9-61cd-45e1-8817-c19bedc7421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/999:\n",
      "  Critic loss: 3.097187042236328\n",
      "  Actor loss: 3.019256114959717\n",
      "Batch 100/999:\n",
      "  Critic loss: 3.1288719177246094\n",
      "  Actor loss: 3.028820037841797\n",
      "Batch 200/999:\n",
      "  Critic loss: 3.113149881362915\n",
      "  Actor loss: 3.041360378265381\n",
      "Batch 300/999:\n",
      "  Critic loss: 3.1476144790649414\n",
      "  Actor loss: 3.053541421890259\n",
      "Batch 400/999:\n",
      "  Critic loss: 3.112830877304077\n",
      "  Actor loss: 2.9973607063293457\n",
      "Batch 500/999:\n",
      "  Critic loss: 3.1262927055358887\n",
      "  Actor loss: 3.0449938774108887\n",
      "Batch 600/999:\n",
      "  Critic loss: 3.1031270027160645\n",
      "  Actor loss: 3.0347092151641846\n",
      "Batch 700/999:\n",
      "  Critic loss: 3.0060291290283203\n",
      "  Actor loss: 2.908104419708252\n",
      "Batch 800/999:\n",
      "  Critic loss: 2.7813453674316406\n",
      "  Actor loss: 2.6836447715759277\n",
      "Batch 900/999:\n",
      "  Critic loss: 2.637681484222412\n",
      "  Actor loss: 2.538935661315918\n",
      "\n",
      "Final Statistics:\n",
      "critic_loss_mean: 2.976257562637329\n",
      "critic_loss_std: 0.26636284589767456\n",
      "critic_loss_min: 1.9644927978515625\n",
      "critic_loss_max: 3.2894484996795654\n",
      "actor_loss_mean: 2.8800742626190186\n",
      "actor_loss_std: 0.2886899411678314\n",
      "actor_loss_min: 1.6733988523483276\n",
      "actor_loss_max: 3.2019906044006348\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (3278470887.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 46\u001b[0;36m\u001b[0m\n\u001b[0;31m    return results\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Compute losses for each batch\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    batch_transitions = jax.tree_util.tree_map(\n",
    "        lambda x: x[batch_idx],\n",
    "        batched_transitions\n",
    "    )\n",
    "    \n",
    "    critic_loss = compute_critic_loss(batch_transitions, critic_params)\n",
    "    actor_loss = compute_actor_loss(\n",
    "        batch_transitions, \n",
    "        actor_params, \n",
    "        critic_params, \n",
    "        alpha_params['log_alpha']\n",
    "    )\n",
    "    \n",
    "    critic_losses.append(critic_loss)\n",
    "    actor_losses.append(actor_loss)\n",
    "    \n",
    "    if batch_idx % 100 == 0:\n",
    "        print(f\"Batch {batch_idx}/{num_batches}:\")\n",
    "        print(f\"  Critic loss: {critic_loss}\")\n",
    "        print(f\"  Actor loss: {actor_loss}\")\n",
    "\n",
    "# Compute statistics\n",
    "critic_losses = jnp.array(critic_losses)\n",
    "actor_losses = jnp.array(actor_losses)\n",
    "\n",
    "results = {\n",
    "    'critic_loss_mean': jnp.mean(critic_losses),\n",
    "    'critic_loss_std': jnp.std(critic_losses),\n",
    "    'critic_loss_min': jnp.min(critic_losses),\n",
    "    'critic_loss_max': jnp.max(critic_losses),\n",
    "    'actor_loss_mean': jnp.mean(actor_losses),\n",
    "    'actor_loss_std': jnp.std(actor_losses),\n",
    "    'actor_loss_min': jnp.min(actor_losses),\n",
    "    'actor_loss_max': jnp.max(actor_losses),\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Statistics:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f85224-b2eb-4989-91db-accffe7e3400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expl-env-jupyter [~/.conda/envs/expl-env-jupyter/]",
   "language": "python",
   "name": "conda_expl-env-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
